{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "An8kHO-S5Z9b",
    "outputId": "639040d1-9fef-4308-92e9-9a668191f329"
   },
   "outputs": [],
   "source": [
    "#Using google drive to store training and testing data.\n",
    "#from google.colab import drive\n",
    "\n",
    "#Retrieving data as objects\n",
    "import pickle\n",
    "#Performing fast computations\n",
    "import numpy as np\n",
    "#Setting uyp Neural Networks\n",
    "import tensorflow as tf\n",
    "#Plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "#Other utilities\n",
    "\n",
    "import cv2\n",
    "#Mounting drive that contains my data.\n",
    "#If you run this notebook, save the training data in a subdirectory called 'HW4-data'. Look at the next cell to better understand the structure of the directories.\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaT-XSXu5wxd"
   },
   "source": [
    "## Training Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = './drive/My Drive/DL Project/'\n",
    "SAVED_MODELS = BASE_PATH + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "mKIJPnOs5b7-",
    "outputId": "61837704-1f50-41e5-9811-de2f8a97b3e2"
   },
   "outputs": [],
   "source": [
    "######### DONT BOTHER WITH THIS FOR NOW\n",
    "#training_set_generator = pickle.load(open( \"./drive/My Drive/DL Project/stickmap.p\", \"rb\" ))\n",
    "#training_set_discriminator1 = pickle.load(open(\"./drive/My Drive/DL Project/train1.p\", \"rb\"))\n",
    "#training_set_discriminator2 = pickle.load(open(\"./drive/My Drive/DL Project/train2.p\", \"rb\"))\n",
    "#training_set_discriminator3 = pickle.load(open(\"./drive/My Drive/DL Project/train3.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6MrPUxOcD5sC"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd9bbPtMHqMj"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xp_jHjgM7v71"
   },
   "outputs": [],
   "source": [
    "#Add layer specific output\n",
    "def generator(z):\n",
    "    with tf.variable_scope(\"GAN/Generator\",reuse=tf.AUTO_REUSE):\n",
    "        #add noise to map. mostly not needed\n",
    "  \n",
    "        #Uncomment all print statements to check output shape \n",
    "        #print(z)\n",
    "    \n",
    "        ######## ENCODING #######\n",
    "  \n",
    "        #Convolution 0 and Max Pooling 0\n",
    "        gen_conv0 = tf.layers.conv2d(z, filters=64, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv0)\n",
    "        gen_pool0 = tf.layers.average_pooling2d(gen_conv0, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool0)\n",
    "  \n",
    "        #Convolution 1 and Max Pooling 1\n",
    "        gen_conv1 = tf.layers.conv2d(gen_pool0, filters=128, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv1)\n",
    "        gen_pool1 = tf.layers.average_pooling2d(gen_conv1, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool1)\n",
    "  \n",
    "        #Convolution 2 and Max Pooling 2\n",
    "        gen_conv2 = tf.layers.conv2d(gen_pool1, filters=256, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv2)\n",
    "        gen_pool2 = tf.layers.average_pooling2d(gen_conv2, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool2)\n",
    "  \n",
    "        #Convolution 3 and Max Pooling 3\n",
    "        gen_conv3 = tf.layers.conv2d(gen_pool2, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv3)\n",
    "        gen_pool3 = tf.layers.average_pooling2d(gen_conv3, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool3)\n",
    "  \n",
    "        #Convolution 4 and Max Pooling 4\n",
    "        gen_conv4 = tf.layers.conv2d(gen_pool3, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv4)\n",
    "        gen_pool4 = tf.layers.average_pooling2d(gen_conv4, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool4)\n",
    "  \n",
    "        #Convolution 5 and Max Pooling 5\n",
    "        gen_conv5 = tf.layers.conv2d(gen_pool4, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv5)\n",
    "        gen_pool5 = tf.layers.average_pooling2d(gen_conv5, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool5)\n",
    "  \n",
    "        #Convolution 6 and Max Pooling 6\n",
    "        gen_conv6 = tf.layers.conv2d(gen_pool5, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_conv6)\n",
    "        gen_pool6 = tf.layers.average_pooling2d(gen_conv6, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(gen_pool6)\n",
    "\n",
    "  \n",
    "        ######## DECODING #######\n",
    "  \n",
    "        #Deconvolution 0\n",
    "        gen_deconv0 = tf.layers.conv2d_transpose(gen_pool6, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv0)\n",
    "\n",
    "        #Deconvolution 1\n",
    "        gen_deconv1 = tf.layers.conv2d_transpose(gen_deconv0, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv1)\n",
    "  \n",
    "        #Deconvolution 2\n",
    "        gen_deconv2 = tf.layers.conv2d_transpose(gen_deconv1, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv2)\n",
    "  \n",
    "        #Deconvolution 3\n",
    "        gen_deconv3 = tf.layers.conv2d_transpose(gen_deconv2, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv3)\n",
    "  \n",
    "        #Deconvolution 4\n",
    "        gen_deconv4 = tf.layers.conv2d_transpose(gen_deconv3, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv4)\n",
    "  \n",
    "        #Deconvolution 5\n",
    "        gen_deconv5 = tf.layers.conv2d_transpose(gen_deconv4, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv5)\n",
    "  \n",
    "        #Deconvolution 6\n",
    "        gen_deconv6 = tf.layers.conv2d_transpose(gen_deconv5, filters=512, kernel_size=(3,3), strides=(2,2), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv6)\n",
    "  \n",
    "        #Deconvolution 7\n",
    "        gen_deconv7 = tf.layers.conv2d_transpose(gen_deconv6, filters=3, kernel_size=(3,3), strides=(1,1), padding='SAME', kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(gen_deconv7)\n",
    "  \n",
    "    \n",
    "        return gen_deconv7  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggf0QoDqfnrM"
   },
   "source": [
    "### Discriminator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UauTa-PfnVE"
   },
   "outputs": [],
   "source": [
    "#Add layer specific output\n",
    "\n",
    "def discriminator_1(z):\n",
    "    with tf.variable_scope(\"GAN/Discriminator1\",reuse=tf.AUTO_REUSE):\n",
    "  \n",
    "        #print(z)\n",
    "        #Uncomment all print statements to check output shapes\n",
    "    \n",
    "    \n",
    "        #Convolution 0 and Max Pooling 0\n",
    "        dis1_conv0 = tf.layers.conv2d(z, filters=64, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv0)\n",
    "        dis1_pool0 = tf.layers.average_pooling2d(dis1_conv0, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool0)\n",
    "  \n",
    "        #Convolution 1 and Max Pooling 1\n",
    "        dis1_conv1 = tf.layers.conv2d(dis1_pool0, filters=128, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv1)\n",
    "        dis1_pool1 = tf.layers.average_pooling2d(dis1_conv1, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool1)\n",
    "  \n",
    "        #Convolution 2 and Max Pooling 2\n",
    "        dis1_conv2 = tf.layers.conv2d(dis1_pool1, filters=256, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv2)\n",
    "        dis1_pool2 = tf.layers.average_pooling2d(dis1_conv2, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool2)\n",
    "  \n",
    "        #Convolution 3 and Max Pooling 3\n",
    "        dis1_conv3 = tf.layers.conv2d(dis1_pool2, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv3)\n",
    "        dis1_pool3 = tf.layers.average_pooling2d(dis1_conv3, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool3)\n",
    "  \n",
    "        #Convolution 4 and Max Pooling 4\n",
    "        dis1_conv4 = tf.layers.conv2d(dis1_pool3, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv4)\n",
    "        dis1_pool4 = tf.layers.average_pooling2d(dis1_conv4, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool4)\n",
    "  \n",
    "        #Convolution 5 and Max Pooling 5\n",
    "        dis1_conv5 = tf.layers.conv2d(dis1_pool4, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv5)\n",
    "        dis1_pool5 = tf.layers.average_pooling2d(dis1_conv5, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool5)\n",
    "  \n",
    "        #Convolution 6 and Max Pooling 6\n",
    "        dis1_conv6 = tf.layers.conv2d(dis1_pool5, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv6)\n",
    "        dis1_pool6 = tf.layers.average_pooling2d(dis1_conv6, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis1_pool6)\n",
    "  \n",
    "        #Final Convolution and Flattening\n",
    "        dis1_conv7 = tf.layers.conv2d(dis1_pool6, filters=1, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_conv7)\n",
    "        dis1_flat = tf.layers.flatten(dis1_conv7)\n",
    "        #print(dis1_flat)\n",
    "        dis1_logit = tf.layers.dense(dis1_flat, 1, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis1_logit)\n",
    "    \n",
    "        return dis1_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WO8lQTbyjHCT"
   },
   "source": [
    "### Discriminator 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNGtnCUyjGxP"
   },
   "outputs": [],
   "source": [
    "#Add layer specific output\n",
    "\n",
    "def discriminator_2(z):\n",
    "    with tf.variable_scope(\"GAN/Discriminator2\",reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "        #print(z)\n",
    "    \n",
    "        #Uncomment all print statements to check output shapes\n",
    "  \n",
    "        #Convolution 0 and Max Pooling 0\n",
    "        dis2_conv0 = tf.layers.conv2d(z, filters=64, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv0)\n",
    "        dis2_pool0 = tf.layers.average_pooling2d(dis2_conv0, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool0)\n",
    "  \n",
    "        #Convolution 1 and Max Pooling 1\n",
    "        dis2_conv1 = tf.layers.conv2d(dis2_pool0, filters=128, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv1)\n",
    "        dis2_pool1 = tf.layers.average_pooling2d(dis2_conv1, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool1)\n",
    "  \n",
    "        #Convolution 2 and Max Pooling 2\n",
    "        dis2_conv2 = tf.layers.conv2d(dis2_pool1, filters=256, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv2)\n",
    "        dis2_pool2 = tf.layers.average_pooling2d(dis2_conv2, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool2)\n",
    "  \n",
    "        #Convolution 3 and Max Pooling 3\n",
    "        dis2_conv3 = tf.layers.conv2d(dis2_pool2, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv3)\n",
    "        dis2_pool3 = tf.layers.average_pooling2d(dis2_conv3, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool3)\n",
    "  \n",
    "        #Convolution 4 and Max Pooling 4\n",
    "        dis2_conv4 = tf.layers.conv2d(dis2_pool3, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv4)\n",
    "        dis2_pool4 = tf.layers.average_pooling2d(dis2_conv4, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool4)\n",
    "  \n",
    "        #Convolution 5 and Max Pooling 5\n",
    "        dis2_conv5 = tf.layers.conv2d(dis2_pool4, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv5)\n",
    "        dis2_pool5 = tf.layers.average_pooling2d(dis2_conv5, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool5)\n",
    "  \n",
    "        #Convolution 6 and Max Pooling 6\n",
    "        dis2_conv6 = tf.layers.conv2d(dis2_pool5, filters=512, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv6)\n",
    "        dis2_pool6 = tf.layers.average_pooling2d(dis2_conv6, pool_size=2, strides=2, padding=\"VALID\")\n",
    "        #print(dis2_pool6)\n",
    "  \n",
    "        #Final Convolution and Flattening\n",
    "        dis2_conv7 = tf.layers.conv2d(dis2_pool6, filters=1, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.relu, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_conv7)\n",
    "        dis2_flat = tf.layers.flatten(dis2_conv7)\n",
    "        #print(dis2_flat)\n",
    "        dis2_logit = tf.layers.dense(dis2_flat, 1, kernel_initializer=tf.variance_scaling_initializer())\n",
    "        #print(dis2_logit)\n",
    "  \n",
    "        return dis2_logit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Bed / Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#Graph setup\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Placeholders\n",
    "d1_data = tf.placeholder(tf.float32, shape=[None,640,640,6]) #img and map\n",
    "d2_data = tf.placeholder(tf.float32, shape=[None,640,640,6])\n",
    "posemap = tf.placeholder(tf.float32, shape=[None,640,640,3])\n",
    "noise = tf.placeholder(tf.float32, shape=[None,640,640,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"GAN/Discriminator1_1/dense/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"GAN/Discriminator2_1/dense/BiasAdd:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Pathways\n",
    "discriminator_1_image_decision = discriminator_1(d1_data)\n",
    "discriminator_2_image_decision = discriminator_2(d2_data)\n",
    "generated_image=generator(tf.concat([noise,posemap], 3))\n",
    "discriminator_1_generated_decision = discriminator_1(tf.concat([generated_image,posemap], 3))\n",
    "discriminator_2_generated_decision = discriminator_2(tf.concat([generated_image,posemap], 3))\n",
    "      #Check shapes\n",
    "print(discriminator_1_generated_decision)\n",
    "print(discriminator_2_generated_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Collecting variables\n",
    "generator_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Generator\")\n",
    "discriminator_1_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator1\")\n",
    "discriminator_2_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#LOSSES\n",
    "\n",
    "# Normal\n",
    "\n",
    "\n",
    "#discriminator_1_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_1_image_decision,labels=tf.ones_like(discriminator_1_image_decision)) + \n",
    "#                                      tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_1_generated_decision, labels=tf.zeros_like(discriminator_1_generated_decision)))\n",
    "#discriminator_2_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_2_image_decision,labels=tf.ones_like(discriminator_2_image_decision)) + \n",
    "#                                      tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_2_generated_decision, labels=tf.zeros_like(discriminator_2_generated_decision)))\n",
    "#generator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_1_generated_decision,labels=tf.ones_like(discriminator_1_generated_decision)) + \n",
    "#                                  tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_2_generated_decision,labels=tf.ones_like(discriminator_2_generated_decision)))\n",
    "\n",
    "\n",
    "#Losses - Wasserstien (add feature matching loss)\n",
    "discriminator_1_loss = tf.reduce_mean(discriminator_1_image_decision) - tf.reduce_mean(discriminator_1_generated_decision)\n",
    "discriminator_2_loss = tf.reduce_mean(discriminator_2_image_decision) - tf.reduce_mean(discriminator_2_generated_decision)\n",
    "generator_loss =  -tf.reduce_mean(discriminator_1_generated_decision) - tf.reduce_mean(discriminator_2_generated_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "discriminator_1_optimizer = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(-discriminator_1_loss, var_list=discriminator_1_variables))\n",
    "discriminator_2_optimizer = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(-discriminator_2_loss, var_list=discriminator_2_variables))\n",
    "generator_optimizer = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(-generator_loss, var_list=generator_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#Gradient clipping\n",
    "clip_discriminator_1_gradient = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in discriminator_1_variables]\n",
    "clip_discriminator_2_gradient = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in discriminator_2_variables]\n",
    "\n",
    "\n",
    "#Add FM loss to Generator- wont take time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "#DUMMY DATA - change these data points - read file during epochs\n",
    "a = np.random.rand(10,640,640,6)\n",
    "b = np.random.rand(10,640,640,3)\n",
    "c = np.random.rand(10,640,640,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[10,64,640,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: GAN/Generator/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](GAN/Generator/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, GAN/Generator/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: sub/_59 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1277_sub\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'GAN/Generator/conv2d/Conv2D', defined at:\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c17fd8bbd1d6>\", line 4, in <module>\n    generated_image=generator(tf.concat([noise,posemap], 3))\n  File \"<ipython-input-3-75e905e1629b>\", line 12, in generator\n    gen_conv0 = tf.layers.conv2d(z, filters=64, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 425, in conv2d\n    return layer.apply(inputs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 805, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 362, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 736, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 186, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 868, in __call__\n    return self.conv_op(inp, filter)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 520, in __call__\n    return self.call(inp, filter)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 204, in __call__\n    name=self.name)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1042, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10,64,640,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: GAN/Generator/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](GAN/Generator/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, GAN/Generator/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: sub/_59 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1277_sub\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10,64,640,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: GAN/Generator/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](GAN/Generator/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, GAN/Generator/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: sub/_59 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1277_sub\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6775c337f273>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md1_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdiscriminator_1_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_1_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_discriminator_1_gradient\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0md1_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposemap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdiscriminator_2_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_2_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_discriminator_2_gradient\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0md1_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposemap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10,64,640,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: GAN/Generator/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](GAN/Generator/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, GAN/Generator/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: sub/_59 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1277_sub\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'GAN/Generator/conv2d/Conv2D', defined at:\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c17fd8bbd1d6>\", line 4, in <module>\n    generated_image=generator(tf.concat([noise,posemap], 3))\n  File \"<ipython-input-3-75e905e1629b>\", line 12, in generator\n    gen_conv0 = tf.layers.conv2d(z, filters=64, kernel_size=3, strides=1, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.variance_scaling_initializer())\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 425, in conv2d\n    return layer.apply(inputs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 805, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 362, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 736, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 186, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 868, in __call__\n    return self.conv_op(inp, filter)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 520, in __call__\n    return self.call(inp, filter)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 204, in __call__\n    name=self.name)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1042, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10,64,640,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: GAN/Generator/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](GAN/Generator/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, GAN/Generator/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: sub/_59 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1277_sub\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "#Solving - put this in loop - write code to save model - ask bhushan. \n",
    "sess.run(tf.initialize_all_variables())\n",
    "for epoch in range(10):\n",
    "    print(epoch)\n",
    "  \n",
    "    _, d1_loss, _ = sess.run([discriminator_1_optimizer, discriminator_1_loss, clip_discriminator_1_gradient], feed_dict={d1_data:a, d2_data:a, posemap:b, noise:c})\n",
    "    print(d1_loss)\n",
    "    _, d2_loss, _ = sess.run([discriminator_2_optimizer, discriminator_2_loss, clip_discriminator_2_gradient], feed_dict={d1_data:a, d2_data:a, posemap:b, noise:c})\n",
    "    print(d2_loss)\n",
    "    _, gen_loss = sess.run([generator_optimizer, generator_loss],feed_dict={d1_data:a, d2_data:a, posemap:b, noise:c} )\n",
    "    print(gen_loss)\n",
    "    \n",
    "    if epoch % save_step == 0:\n",
    "        saver.save(sess, SAVED_MODELS + 'model_' + ('0000' + str(epoch))[-3:] + '.ckpt')\n",
    "\n",
    "#Printing section\n",
    "#print('=============================================')\n",
    "#print(discriminator_1_image_decision)\n",
    "#print(discriminator_2_image_decision)\n",
    "#print(generated_image)\n",
    "#print(discriminator_1_generated_decision)\n",
    "#print(discriminator_2_generated_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "x5omAzdnPo8l",
    "outputId": "54011987-4466-4c43-fe03-47f5f69a9b46"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNI7uFx3QjBT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "DL Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
